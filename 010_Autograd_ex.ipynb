{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "010-Autograd_ex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmLywutrdWTaEjyPkkIQfG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kouroshkarimi/Pytorch-Introducing-to-Deep-Learning/blob/main/010_Autograd_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1PySNDz_8r"
      },
      "source": [
        "# Autograd: automatic differentiation\r\n",
        "\r\n",
        "The ``autograd`` package provides automatic differentiation for all operations\r\n",
        "on Tensors. It is a define-by-run framework, which means that your backprop is\r\n",
        "defined by how your code is run, and that every single iteration can be\r\n",
        "different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0flEygOa0Dyo"
      },
      "source": [
        "# Import package\r\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSZ4Ry9Y0J1T"
      },
      "source": [
        "x = torch.tensor([[2, 1],[2, 1],[3, 1]], requires_grad=True, dtype = torch.float32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiYasMAY0XTZ",
        "outputId": "0dbcd6ac-f7d5-4778-88e1-b46fcb4934da"
      },
      "source": [
        "print(\"[INFO]\")\r\n",
        "print(\"The size of x is {}\".format(x.size()))\r\n",
        "print(\"The dimentiona of x is {}\".format(x.dim()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]\n",
            "The size of x is torch.Size([3, 2])\n",
            "The dimentiona of x is 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BxezQIa0fbd",
        "outputId": "307eda51-5f93-4433-a110-0ea4e156902a"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 1.],\n",
            "        [2., 1.],\n",
            "        [3., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEyak0SC1C-Z"
      },
      "source": [
        "y = x + 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigOH90x1Hf3",
        "outputId": "ffb7199f-bfb6-4562-f88d-329206852112"
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7f6c47468e10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sne-ocXt2hKM"
      },
      "source": [
        "### Thereâ€™s one more class which is very important for autograd implementation - a Function. Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each variable has a .grad_fn attribute that references a function that has created a function (except for Tensors created by the user - these have None as .grad_fn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yuL--Ps1KVw",
        "outputId": "f4f76dd6-d637-4de6-ee85-21ad7c511e10"
      },
      "source": [
        "print(x.grad_fn)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF0TS6QG1UAN",
        "outputId": "4696a18c-deb4-4cd5-e93e-ae0db8df6565"
      },
      "source": [
        "y.grad_fn.next_functions[0][0].variable"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o7oBZl42U4C"
      },
      "source": [
        "In autograd, if any input Tensor of an operation has requires_grad=True, the computation will be tracked. After computing the backward pass, a gradient w.r.t. this tensor is accumulated into .grad attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CeIyTQY2WUC"
      },
      "source": [
        "z = 4 * (y ** 2)\r\n",
        "a = z.mean()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "kBv57LDY224Q",
        "outputId": "5413d18d-c7df-40fc-a6a0-6eabe5832609"
      },
      "source": [
        "# We can visualize Computational graph with torchviz package (in google colab dont work you must install it)\r\n",
        "import torchviz as tv\r\n",
        "tv.make_dot(a)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2f9d4ba63874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We can visualize Computational graph with torchviz package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDKxDVl3jTM"
      },
      "source": [
        "## Gradients\r\n",
        "* we can calculate all bakcward partial diffrentials with bakcward()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcXtnyFd3xJQ"
      },
      "source": [
        "a.backward()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmFl_r0a3ya4",
        "outputId": "4896ebec-2699-498a-911d-8fc928f65036"
      },
      "source": [
        "# calculating da/dx (index by index)\r\n",
        "x.grad"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.6667, 5.3333],\n",
              "        [6.6667, 5.3333],\n",
              "        [8.0000, 5.3333]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BIt-NUG5FZC"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW5PRDxi36Se",
        "outputId": "1d59cae5-1faf-4a3e-c0cd-0ebc7a8c9c6f"
      },
      "source": [
        "# This variable decides the tensor's range below\r\n",
        "n = 3\r\n",
        "# Both x and w that allows gradient accumulation\r\n",
        "x = torch.arange(1., n + 1, requires_grad=True)\r\n",
        "w = torch.ones(n, requires_grad=True)\r\n",
        "z = w @ x\r\n",
        "z.backward()\r\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n",
            "tensor([1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnbgoprk5LTf",
        "outputId": "c884bf96-6d04-4a01-cbb6-3931e732dfe6"
      },
      "source": [
        "# Only w that allows gradient accumulation\r\n",
        "x = torch.arange(1., n + 1)\r\n",
        "w = torch.ones(n, requires_grad=True)\r\n",
        "z = w @ x\r\n",
        "z.backward()\r\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2CE4IhY5NoF",
        "outputId": "d2776bc8-a8a9-4792-e7d4-04b9757c0587"
      },
      "source": [
        "x = torch.arange(1., n + 1)\r\n",
        "w = torch.ones(n, requires_grad=True)\r\n",
        "\r\n",
        "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\r\n",
        "with torch.no_grad():\r\n",
        "    z = w @ x\r\n",
        "\r\n",
        "try:\r\n",
        "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\r\n",
        "except RuntimeError as e:\r\n",
        "    print('RuntimeError!!! >:[')\r\n",
        "    print(e)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RuntimeError!!! >:[\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCS6tA675eSa"
      },
      "source": [
        "## More stuff\r\n",
        "\r\n",
        "* You can see renfrence of this codes below and you can do more and see more example there.\r\n",
        "\r\n",
        "Documentation of the automatic differentiation package is at\r\n",
        "http://pytorch.org/docs/autograd."
      ]
    }
  ]
}